{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40da419b",
   "metadata": {},
   "source": [
    "# COVID-19 Outcome Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac26b52b",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "MLP\n",
    "\n",
    "- Data analysis\n",
    "- Data preprocessing and normalization\n",
    "- Data split\n",
    "- Configuration\n",
    "- Experiment tracking\n",
    "- Experiments - meaningful based on the results of previous experiments\n",
    "- Including improvement techniques (e.g. Dropout, Normalization layers, Skip Connections, Bottleneck Layers, …)\n",
    "- Hyperparameter search (grid/random) / sweep #optional\n",
    "- Results and evaluation metrics\n",
    "- Clear code\n",
    "- Markdown documentation and comments\n",
    "- Final presentation of projects\n",
    "- Effort on consultations\n",
    "\n",
    "# OUR TODO:\n",
    "\n",
    "- download dataset using pandas ? easier because of we can just use link but maybe everyone will use pandas -done\n",
    "- good enough data analysis that is seen in jupyter notebook, not just from reading dataset - done\n",
    "- correlation chart - chart of correlation of columns between themselves - done\n",
    "- outliers - what to do with them ?\n",
    "- counts of data in each column and plots of them -done\n",
    "- normalize age 0-1 young/old ? min max or z score, mean have 0 deviation has 1\n",
    "- warning if something has 100% correlation we should do something with it\n",
    "- better -1 is False; 0 - not state /dont know; 1 - True\n",
    "- think about combining weird features or if 20% are not state try to remove them\n",
    "- vantby tracking experiments\n",
    "- focus on binary prediction with binary cross entropy on only testing (zamerane na binarnu predikciu binary cross entropy na testovacej dat metriky netrenujem )\n",
    "- datasplit for training validation and testing DF 70 10 20 and check if decreasing both training and validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40261e",
   "metadata": {},
   "source": [
    "End-to-end pipeline covering EDA, preprocessing, stratified splits, imbalance-aware PyTorch models, experiment tracking, and final evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb75b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275cbb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"dataset.csv\")\n",
    "BATCH_SIZE = 1024\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1655881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Shape of dataset: {raw_df.shape}\")\n",
    "display(raw_df.head())\n",
    "\n",
    "numeric_df = raw_df.select_dtypes(include=[\"number\"])\n",
    "\n",
    "raw_df.info()\n",
    "display(raw_df.count(numeric_only=True).to_frame(name=\"count\"))\n",
    "print(\"Dataset shape:\", raw_df.shape)\n",
    "\n",
    "for col in numeric_df.columns:\n",
    "    series = numeric_df[col]\n",
    "    counts = series.value_counts(dropna=False).sort_index()\n",
    "    x = range(len(counts))\n",
    "    labels = counts.index.astype(str)\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.bar(x, counts.values, color=\"steelblue\", alpha=0.9, width=0.8)\n",
    "    plt.title(f\"Count per value — {col}\", fontsize=12, pad=8)\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    if len(labels) > 40:\n",
    "        step = max(1, len(labels) // 40)\n",
    "        plt.xticks(list(x)[::step], labels[::step], rotation=45, ha=\"right\")\n",
    "    else:\n",
    "        plt.xticks(x, labels, rotation=45, ha=\"right\")\n",
    "    plt.grid(True, linestyle=\"--\", axis=\"y\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "corr = numeric_df.corr()\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    fmt=\".1f\",\n",
    "    square=True,\n",
    "    linewidths=1,\n",
    "    cbar_kws={\"shrink\": 0.7, \"label\": \"Correlation\"}\n",
    ")\n",
    "plt.title(\"Correlation Heatmap of Numeric Attributes\", fontsize=14, pad=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c06d15",
   "metadata": {},
   "source": [
    "## Exploratory analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1441826",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(raw_df.describe(include='all').transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4970faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_series = raw_df['CLASIFFICATION_FINAL'].isin([1, 2, 3]).astype(int)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "target_series.value_counts(normalize=True).mul(100).reindex([0, 1]).plot(\n",
    "    kind='bar', ax=axes[0], color=['steelblue', 'tomato']\n",
    ")\n",
    "axes[0].set_title('Target distribution (%)')\n",
    "axes[0].set_xlabel('HAS_COVID')\n",
    "axes[0].set_ylabel('Percent')\n",
    "\n",
    "age_sample = raw_df['AGE'].clip(0, 100)\n",
    "axes[1].hist(age_sample, bins=30, color='slategray', alpha=0.9)\n",
    "axes[1].set_title('Age distribution (trimmed 0-100)')\n",
    "axes[1].set_xlabel('Age')\n",
    "axes[1].set_ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b290a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = raw_df.select_dtypes(include=['number']).columns\n",
    "sample_for_corr = raw_df[numeric_cols].sample(n=min(2000, len(raw_df)), random_state=42)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sample_for_corr.corr(), cmap='coolwarm', linewidths=0.1, center=0)\n",
    "plt.title('Correlation heatmap (raw numeric sample)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09604e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df = raw_df.copy()\n",
    "print('Sample before preprocessing:')\n",
    "display(demo_df.iloc[:30])\n",
    "\n",
    "demo_df.columns = demo_df.columns.str.strip()\n",
    "demo_df.rename(columns={'DATE_DIED': 'DEAD'}, inplace=True)\n",
    "med_unit_original = demo_df['MEDICAL_UNIT'].copy()\n",
    "classif_original = demo_df['CLASIFFICATION_FINAL'].copy()\n",
    "\n",
    "demo_df['DEAD'] = np.where(demo_df['DEAD'] == '9999-99-99', 2, 1)\n",
    "demo_df.loc[demo_df['SEX'] == 1, 'PREGNANT'] = 0\n",
    "binary_cols = ['USMER','SEX','INTUBED','PNEUMONIA','PREGNANT','DIABETES','COPD','ASTHMA','INMSUPR','HIPERTENSION','OTHER_DISEASE','CARDIOVASCULAR','OBESITY','RENAL_CHRONIC','TOBACCO','ICU','DEAD']\n",
    "demo_df[binary_cols] = demo_df[binary_cols].replace(2, 0)\n",
    "demo_df[binary_cols] = demo_df[binary_cols].replace(97, 0.5)\n",
    "demo_df[binary_cols] = demo_df[binary_cols].replace(98, 0.5)\n",
    "demo_df['MEDICAL_UNIT'] = med_unit_original\n",
    "demo_df['CLASIFFICATION_FINAL'] = classif_original\n",
    "demo_df.rename(columns={'SEX': 'IS_MALE'}, inplace=True)\n",
    "demo_df['CLASIFFICATION_FINAL'] = demo_df['CLASIFFICATION_FINAL'].replace([1,2,3], 1)\n",
    "demo_df['CLASIFFICATION_FINAL'] = demo_df['CLASIFFICATION_FINAL'].replace([4,5,6,7], 0)\n",
    "demo_df.rename(columns={'CLASIFFICATION_FINAL': 'HAS_COVID'}, inplace=True)\n",
    "print('Sample after key preprocessing steps:')\n",
    "display(demo_df.iloc[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7167b",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03adbc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = frame.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.rename(columns={\"SEX\": \"IS_MALE\"}, inplace=True)\n",
    "    valid = df[\"CLASIFFICATION_FINAL\"].isin([1, 2, 3, 4, 5, 6])\n",
    "    df = df.loc[valid].copy()\n",
    "\n",
    "    df[\"DEAD\"] = np.where(df[\"DATE_DIED\"] == \"9999-99-99\", 0, 1)\n",
    "\n",
    "    binary_cols = [\n",
    "        \"USMER\",\n",
    "        \"IS_MALE\",\n",
    "        \"INTUBED\",\n",
    "        \"PNEUMONIA\",\n",
    "        \"PREGNANT\",\n",
    "        \"DIABETES\",\n",
    "        \"COPD\",\n",
    "        \"ASTHMA\",\n",
    "        \"INMSUPR\",\n",
    "        \"HIPERTENSION\",\n",
    "        \"OTHER_DISEASE\",\n",
    "        \"CARDIOVASCULAR\",\n",
    "        \"OBESITY\",\n",
    "        \"RENAL_CHRONIC\",\n",
    "        \"TOBACCO\",\n",
    "        \"ICU\",\n",
    "        \"DEAD\",\n",
    "    ]\n",
    "    replace_map = {1: 1, 2: 0, 3: np.nan, 97: np.nan, 98: np.nan, 99: np.nan}\n",
    "    df[binary_cols] = df[binary_cols].replace(replace_map)\n",
    "    df.loc[df[\"IS_MALE\"] == 1, \"PREGNANT\"] = 0\n",
    "\n",
    "    df[\"IS_HOSPITALIZED\"] = df[\"PATIENT_TYPE\"].map({1: 0, 2: 1}).fillna(0)\n",
    "    df.drop(columns=[\"PATIENT_TYPE\"], inplace=True)\n",
    "\n",
    "    df[\"HAS_COVID\"] = df[\"CLASIFFICATION_FINAL\"].replace({1: 1, 2: 1, 3: 1, 4: 0, 5: 0, 6: 0})\n",
    "    df.drop(columns=[\"CLASIFFICATION_FINAL\", \"DATE_DIED\"], inplace=True)\n",
    "\n",
    "    df[binary_cols] = df[binary_cols].fillna(df[binary_cols].median())\n",
    "\n",
    "    risk_cols = [\n",
    "        \"DIABETES\",\n",
    "        \"COPD\",\n",
    "        \"ASTHMA\",\n",
    "        \"INMSUPR\",\n",
    "        \"HIPERTENSION\",\n",
    "        \"OTHER_DISEASE\",\n",
    "        \"CARDIOVASCULAR\",\n",
    "        \"OBESITY\",\n",
    "        \"RENAL_CHRONIC\",\n",
    "    ]\n",
    "    df[\"CHRONIC_COUNT\"] = df[risk_cols].sum(axis=1)\n",
    "    df[\"MULTI_MORBID\"] = (df[\"CHRONIC_COUNT\"] >= 2).astype(int)\n",
    "    df[\"RISK_OBESE_SMOKER\"] = ((df[\"OBESITY\"] == 1) | (df[\"TOBACCO\"] == 1)).astype(int)\n",
    "\n",
    "    df[\"AGE_BUCKET\"] = (\n",
    "        pd.cut(df[\"AGE\"], bins=[0, 30, 45, 60, 75, 120], labels=False, include_lowest=True)\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df[\"AGE\"] = scaler.fit_transform(df[[\"AGE\"]])\n",
    "\n",
    "    df = pd.get_dummies(df, columns=[\"MEDICAL_UNIT\"], prefix=\"MED_UNIT\", drop_first=True)\n",
    "    df = df.dropna()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b809b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = preprocess_data(raw_df)\n",
    "print(f\"Processed shape: {model_df.shape}\")\n",
    "display(model_df.head())\n",
    "feature_cols = [col for col in model_df.columns if col != \"HAS_COVID\"]\n",
    "print(f\"Feature count: {len(feature_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_numeric = model_df.select_dtypes(include=['number'])\n",
    "corr_processed = processed_numeric.corr()\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(\n",
    "    corr_processed,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt='.1f',\n",
    "    square=True,\n",
    "    linewidths=1,\n",
    "    cbar_kws={'shrink': 0.7, 'label': 'Correlation'}\n",
    ")\n",
    "plt.title('Correlation Heatmap after Preprocessing', fontsize=14, pad=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6ff099",
   "metadata": {},
   "source": [
    "## Data split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(\n",
    "    model_df,\n",
    "    test_size=0.3,\n",
    "    stratify=model_df[\"HAS_COVID\"],\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=2 / 3,\n",
    "    stratify=temp_df[\"HAS_COVID\"],\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "def describe_split(name, frame):\n",
    "    share = frame[\"HAS_COVID\"].mean() * 100\n",
    "    print(f\"{name:<10s} n={len(frame):,} | positive={share:.2f}% | negative={100 - share:.2f}%\")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "describe_split(\"Train\", train_df)\n",
    "describe_split(\"Val\", val_df)\n",
    "describe_split(\"Test\", test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9440c89",
   "metadata": {},
   "source": [
    "## Torch datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d179fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_to_dataset(frame):\n",
    "    features = frame[feature_cols].to_numpy(dtype=np.float32)\n",
    "    labels = frame[\"HAS_COVID\"].to_numpy(dtype=np.float32)\n",
    "    return TensorDataset(torch.from_numpy(features), torch.from_numpy(labels))\n",
    "\n",
    "train_dataset = frame_to_dataset(train_df)\n",
    "val_dataset = frame_to_dataset(val_df)\n",
    "test_dataset = frame_to_dataset(test_df)\n",
    "\n",
    "class_counts = train_df[\"HAS_COVID\"].value_counts().to_dict()\n",
    "weights = train_df[\"HAS_COVID\"].map(lambda value: 1.0 / class_counts[value]).to_numpy(dtype=np.float32)\n",
    "sampler = WeightedRandomSampler(weights=torch.from_numpy(weights), num_samples=len(weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "positive = float((train_df[\"HAS_COVID\"] == 1).sum())\n",
    "negative = float((train_df[\"HAS_COVID\"] == 0).sum())\n",
    "pos_weight_value = negative / max(positive, 1.0)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight_value], dtype=torch.float32, device=device))\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Sample batch → X: {sample_batch[0].shape}, y: {sample_batch[1].shape}, pos_weight={pos_weight_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d831c",
   "metadata": {},
   "source": [
    "## Model and helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788726ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for hidden in hidden_layers:\n",
    "            layers.append(nn.Linear(prev, hidden))\n",
    "            layers.append(nn.BatchNorm1d(hidden))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = hidden\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "\n",
    "def metrics_from_probs(probs, labels, threshold=0.5):\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, zero_division=0),\n",
    "        \"f1\": f1_score(labels, preds, zero_division=0),\n",
    "        \"pr_auc\": average_precision_score(labels, probs),\n",
    "    }\n",
    "    prec_curve, rec_curve, _ = precision_recall_curve(labels, probs)\n",
    "    mask = prec_curve >= 0.7\n",
    "    metrics[\"recall_at_70_precision\"] = float(rec_curve[mask].max()) if mask.any() else 0.0\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    losses, probs, labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            losses.append(loss.item())\n",
    "            probs.append(torch.sigmoid(logits).cpu())\n",
    "            labels.append(yb.cpu())\n",
    "    prob_array = torch.cat(probs).numpy()\n",
    "    label_array = torch.cat(labels).numpy()\n",
    "    metrics = metrics_from_probs(prob_array, label_array)\n",
    "    metrics[\"roc_auc\"] = roc_auc_score(label_array, prob_array)\n",
    "    metrics[\"loss\"] = float(np.mean(losses))\n",
    "    return metrics, prob_array, label_array\n",
    "\n",
    "\n",
    "def find_best_threshold(probs, labels):\n",
    "    best_thr = 0.5\n",
    "    best_score = 0.0\n",
    "    for thr in np.linspace(0.1, 0.9, 17):\n",
    "        score = f1_score(labels, (probs >= thr).astype(int), zero_division=0)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_thr = float(thr)\n",
    "    return best_thr, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    model = CovidMLP(len(feature_cols), config[\"hidden_layers\"], config[\"dropout\"]).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "    #init wandb each training with new config\n",
    "    wandb_run = wandb.init(project=\"covid-19\", config=config, reinit=False)\n",
    "    wandb_run.watch(model, log=\"all\")\n",
    "    best_state = None\n",
    "    best_snapshot = None\n",
    "    best_score = -np.inf\n",
    "    history = []\n",
    "    for epoch in range(1, config[\"epochs\"] + 1):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "        val_metrics, val_probs, val_labels = evaluate_model(model, val_loader)\n",
    "        scheduler.step(val_metrics[\"loss\"])\n",
    "        history.append(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": float(np.mean(epoch_losses)),\n",
    "                **{k: v for k, v in val_metrics.items()},\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"{config['name']:<10s} epoch {epoch:02d} \"\n",
    "            f\"train_loss={history[-1]['train_loss']:.3f} val_loss={val_metrics['loss']:.3f} val_f1={val_metrics['f1']:.3f}\"\n",
    "        )\n",
    "        #every epoch log training_loss, val_loss, val_f1 and learning_rate\n",
    "        wandb_run.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": float(np.mean(epoch_losses)),\n",
    "                \"val_loss\": val_metrics[\"loss\"],\n",
    "                \"val_f1\": val_metrics[\"f1\"],\n",
    "                \"learning_rate\": optimizer.param_groups[0][\"lr\"]\n",
    "            }\n",
    "        )\n",
    "        if val_metrics[\"f1\"] > best_score:\n",
    "            best_score = val_metrics[\"f1\"]\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            best_snapshot = (val_metrics.copy(), val_probs.copy(), val_labels.copy())\n",
    "    model.load_state_dict(best_state)\n",
    "    wandb_run.finish()\n",
    "    return model, history, best_snapshot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118d2bcc",
   "metadata": {},
   "source": [
    "## Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca11e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_grid = [\n",
    "    {\"name\": \"baseline\", \"hidden_layers\": [128, 64, 32], \"dropout\": 0.3, \"lr\": 3e-4, \"weight_decay\": 1e-4, \"epochs\": 10},\n",
    "    {\"name\": \"compact\", \"hidden_layers\": [64, 32], \"dropout\": 0.25, \"lr\": 5e-4, \"weight_decay\": 5e-5, \"epochs\": 12},\n",
    "    {\"name\": \"wide\", \"hidden_layers\": [256, 128, 64], \"dropout\": 0.4, \"lr\": 2e-4, \"weight_decay\": 1e-4, \"epochs\": 12},\n",
    "]\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "for cfg in experiment_grid:\n",
    "    model, history, snapshot = train_model(cfg)\n",
    "    metrics_at_05, val_probs, val_labels = snapshot\n",
    "    best_thr, _ = find_best_threshold(val_probs, val_labels)\n",
    "    tuned_metrics = metrics_from_probs(val_probs, val_labels, threshold=best_thr)\n",
    "    tuned_metrics[\"roc_auc\"] = metrics_at_05[\"roc_auc\"]\n",
    "    tuned_metrics[\"loss\"] = metrics_at_05[\"loss\"]\n",
    "    experiment_results.append(\n",
    "        {\n",
    "            \"name\": cfg[\"name\"],\n",
    "            \"config\": cfg,\n",
    "            \"state_dict\": deepcopy(model.state_dict()),\n",
    "            \"threshold\": best_thr,\n",
    "            \"val_metrics\": tuned_metrics,\n",
    "            \"history\": history,\n",
    "        }\n",
    "    )\n",
    "\n",
    "results_table = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"experiment\": result[\"name\"],\n",
    "            \"val_f1\": result[\"val_metrics\"][\"f1\"],\n",
    "            \"val_pr_auc\": result[\"val_metrics\"][\"pr_auc\"],\n",
    "            \"val_recall_at_70_precision\": result[\"val_metrics\"][\"recall_at_70_precision\"],\n",
    "            \"threshold\": result[\"threshold\"],\n",
    "        }\n",
    "        for result in experiment_results\n",
    "    ]\n",
    ").sort_values(by=\"val_f1\", ascending=False)\n",
    "\n",
    "print(\"Validation summary:\")\n",
    "display(results_table.reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_metrics = [\"train_loss\", \"loss\", \"f1\"]\n",
    "for result in experiment_results:\n",
    "    history_df = pd.DataFrame(result[\"history\"])\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    for metric in history_metrics:\n",
    "        if metric in history_df.columns:\n",
    "            plt.plot(history_df[\"epoch\"], history_df[metric], label=metric.replace(\"_\", \" \"))\n",
    "    plt.title(f\"Training history - {result['name']}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2fd4e",
   "metadata": {},
   "source": [
    "## Test evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b123fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_experiment = max(experiment_results, key=lambda item: item[\"val_metrics\"][\"f1\"])\n",
    "best_cfg = best_experiment[\"config\"]\n",
    "best_threshold = best_experiment[\"threshold\"]\n",
    "\n",
    "best_model = CovidMLP(len(feature_cols), best_cfg[\"hidden_layers\"], best_cfg[\"dropout\"]).to(device)\n",
    "best_model.load_state_dict(best_experiment[\"state_dict\"])\n",
    "\n",
    "test_metrics_raw, test_probs, test_labels = evaluate_model(best_model, test_loader)\n",
    "test_metrics = metrics_from_probs(test_probs, test_labels, threshold=best_threshold)\n",
    "test_metrics[\"roc_auc\"] = test_metrics_raw[\"roc_auc\"]\n",
    "test_metrics[\"loss\"] = test_metrics_raw[\"loss\"]\n",
    "\n",
    "print(\"Test metrics with tuned threshold:\")\n",
    "for key in [\"loss\", \"roc_auc\", \"pr_auc\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"recall_at_70_precision\"]:\n",
    "    print(f\"  {key}: {test_metrics[key]:.4f}\")\n",
    "\n",
    "binary_preds = (test_probs >= best_threshold).astype(int)\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(test_labels, binary_preds, digits=4, zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(test_labels, binary_preds)\n",
    "cm_df = pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Pred 0\", \"Pred 1\"])\n",
    "print(\"Confusion matrix:\")\n",
    "display(cm_df)\n",
    "#using best model and config to log confusion matrix to wandb\n",
    "wandb_run = wandb.init(project=\"covid-19\", config=best_cfg, reinit=True)\n",
    "wandb_run.log({\"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "        preds=binary_preds.tolist(),\n",
    "        y_true=test_labels.tolist(),\n",
    "        title=\"Confusion Matrix of best Model\"\n",
    "    )})\n",
    "wandb_run.finish()\n",
    "\n",
    "precisions, recalls, _ = precision_recall_curve(test_labels, test_probs)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(recalls, precisions, label=\"PR curve\")\n",
    "plt.scatter(test_metrics[\"recall\"], test_metrics[\"precision\"], color=\"red\", label=\"Chosen threshold\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Test precision-recall curve\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
